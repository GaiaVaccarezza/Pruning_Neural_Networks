During the course of Machine Learning - part II we were asked to study a relevant paper on Neural Networks and its subsequent evolution. 

I chose "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks" by Frankle & Garbin. The paper is very innovative and valuable as it provides a way to prune Neural Networks, 
which are known to be very big statical models. Their gigantic structure leads to very prohibitive cost of training and mantainance. For example, OpenAIâ€™s GPT-3 has been calculated to have a training cost of $4.6M
using the lowest-cost cloud GPU on the market. Therefore, a efficent and effective technique to reduce their size without jeopardizing their accuracy would be very valuable. 

The paper is pioneer in this topic and even if the results are very good there are still some difficulties to overcome. Therefore, I looked for more recent literature that provides some solutions. 
